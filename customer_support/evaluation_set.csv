"What single sign-on (SSO) providers does Atlan support?","Atlan supports SSO authentication using SAML 2.0 for the following providers: Azure AD, Google, JumpCloud, Okta, OneLogin, and Custom IdP."
"How does Atlan use tags and policies to manage access to sensitive data?","Atlan uses tags and policies by allowing users to define an access policy that restricts access to assets based on a specific tag, such as 'PII'. These tags can be configured to automatically propagate downstream to any derived assets, ensuring that the access control is also automatically applied to them."
"How does Atlan handle data and metadata persistence?","Atlan is a virtualized solution that does not move or store your data. It crawls metadata from upstream sources and stores that metadata in a secure VPC, while data queries are pushed directly to the existing storage layers."
"Where does Atlan store asset metadata and user data?","Atlan stores asset metadata in Apache Atlas, Elasticsearch, and Cassandra. It stores user data, including roles and groups, in its own PostgreSQL database."
"What permissions are required when generating a token for crawling dbt in Atlan, and how do they differ between dbt Cloud Team and Enterprise plans?","When generating a Service Account Token, Team plans require Read-only access to all projects, while Enterprise plans require Job Viewer access. For a Personal Access Token, the user creating it must have Job Viewer access to all relevant projects."
"What is the warning mentioned in the text regarding dbt crawler workflows in Atlan?","The text warns that User API tokens will be deprecated by October 22, 2024. If you have existing dbt crawler workflows in Atlan using these tokens, you must modify their configuration with updated credentials, such as a service account or a personal access token, to avoid encountering errors."
"What is Atlan's process for disaster recovery if a tenant needs to be recreated?","In case of a disaster, Atlan automates the recreation of a tenant by onboarding a new one, restoring data from the last backup using Argo Workflows, scaling down the old tenant, updating the domain and Cloudflare records, with the entire process taking around 3-4 hours."
"How does Atlan manage backups for its tenants?","Atlan performs an automated, full backup of each tenant daily, storing it encrypted in cloud storage with a retention period of 15 days. It backs up components like Elasticsearch, Cassandra, and Postgres, and monitors for any failures."
"What are the Recovery Time Objective (RTO) and Recovery Point Objective (RPO) for Atlan?","For all critical applications, Atlan's RTO is less than 3 hours. Since backups are performed daily, its RPO is 24 hours in a worst-case scenario."
"What is the specific condition that dbt assets must meet to be crawled by Atlan?","A dbt asset must be in the 'applied' or 'built' state, which means it must have been part of a successful run. Assets that are only defined in project files but have not been executed will not be included."
"After crawling dbt, what are some of the dbt-specific filters and custom metadata Atlan uses to enrich assets?","Atlan enriches dbt assets by providing dbt-specific filters for discovery, such as by 'Test status', 'Job status', and 'Project name'. It also populates custom metadata, syncing dbt tags and mapping properties like 'alias' and 'uniqueId' to the assets in Atlan."
"How does Atlan handle data when a user runs a query, and is the data stored in Atlan?","Atlan does not store or cache data. When a query is run, it pushes a SQL query directly to the underlying source database. The results are encrypted in transit, and Atlan applies access policies before displaying them."
"Is it possible to disable the data querying feature in Atlan for all users, and who can perform this action?","Yes, it is possible to block all users from querying data across all assets. An admin user can perform this action by toggling the 'Insights' option off from the admin workspace."
"What are the steps and prerequisites for enabling Atlan event logs in an AWS S3 bucket?","To enable event logs in AWS, the customer's S3 bucket must have versioning enabled, and they must provide their account ID, bucket name, and region. Atlan then provides a bucket policy for the customer to attach, after which Atlan support completes the final configuration."
"How does Atlan enable event log syncing for Google Cloud Platform (GCP), and how does the timing differ for GCS?","For GCP, Atlan uses a Logs Router. The customer provides a destination, Atlan creates a sink and a service account, and the customer must then configure permissions for that account. Sinks to Google Cloud Storage (GCS) buckets are processed hourly, while other destination types are processed in real time."
"What are the key prerequisites you need to set up before using the Atlan GitHub Action for impact analysis?","You must create an Atlan API token and assign it a persona with the necessary metadata policies (for example, Read/Update on dbt assets and Read on downstream connections). You also need to grant Read and write permissions to the default GITHUB_TOKEN in your repository."
"How can you resolve an issue where the Atlan GitHub action fetches a dbt model from an incorrect environment?","To ensure the action fetches a model from the correct environment, you can use the `DBT_ENVIRONMENT_BRANCH_MAP` input in your workflow file to map the GitHub branch to a specific dbt environment, for example, mapping the 'main' branch to 'dbt-prod'."
"What are the main steps to generate and securely provide troubleshooting files to Atlan support using Google Chrome?","First, open the Developer Tools, start recording in the 'Network' tab, and then replicate the issue. Next, save the console log from the 'Console' tab and the HAR file from the 'Network' tab. Finally, it is recommended to edit the HAR file in a text editor to remove any sensitive information like passwords before uploading both files."
"How can you ensure that column data types are correctly displayed for both dbt Cloud and dbt Core models in Atlan?","For dbt Cloud, you must enable the 'Generate docs on run' option for at least one job run in every environment. For dbt Core, you need to upload the `manifest.json` and `catalog.json` files that are generated by the `dbt docs generate` command."
"What are two reasons some dbt tests might be missing or why the test count might differ between Atlan and dbt?","Some dbt tests might be missing because Atlan does not support tests with an auto-generated `unique_id` longer than 32,000 characters. Additionally, the test count can differ because Atlan fetches assets from the 'applied state' of an environment, which may not match the asset count in the manifest files from individual job runs."