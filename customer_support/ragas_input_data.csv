question,ground_truth,answer,contexts
What single sign-on (SSO) providers does Atlan support?,"Atlan supports SSO authentication using SAML 2.0 for the following providers: Azure AD, Google, JumpCloud, Okta, OneLogin, and Custom IdP.",Atlan currently supports the following SSO providers: Azure AD and Okta (Source: https://docs.atlan.com/platform/concepts/authentication-and-authorization).,"['However, Atlan does not recommend using basic authentication. Instead, admins should configure and enforce SSO authentication.\nSSO authentication\u200b\nSSO using SAML 2.0\u200b\nAtlan supports single sign-on (SSO), allowing admins to configure SSO authentication.\nAtlan currently supports the following SSO providers:', 'What type of user provisioning does Atlan support for SSO integrations? | Atlan Documentation', 'Azure AD\nOkta\n\nAtlan also supports just-in-time (JIT) user provisioning and automated assignment of user groups during every login for several SSO providers. Once a user is deprovisioned in your SSO provider, the user will not be able to log into Atlan. Since the user profile in Atlan is not deleted automatically, you will also need to disable the user.Tags:integrationconnectorsfaq-integrationsPreviousCan we use a Microsoft SSO login?NextWhen does Atlan become a personal data processor or subprocessor?Copyright © 2025 Atlan Pte. Ltd.', 'Can we use a Microsoft SSO login? | Atlan Documentation']"
How does Atlan use tags and policies to manage access to sensitive data?,"Atlan uses tags and policies by allowing users to define an access policy that restricts access to assets based on a specific tag, such as 'PII'. These tags can be configured to automatically propagate downstream to any derived assets, ensuring that the access control is also automatically applied to them.","Atlan utilizes tags and policies to manage access to sensitive data through a combination of organized classification and granular access controls. Admin users can create tags to classify data assets based on important characteristics, allowing for better organization and management (Source: https://docs.atlan.com/product/capabilities/governance/tags/concepts/what-are-tags). These tags can be used to group different assets together and apply specific access policies to those groups, facilitating compliance with regulations such as GDPR and CCPA by identifying assets that require special handling (Source: https://docs.atlan.com/product/capabilities/governance/tags/concepts/what-are-tags). 

Additionally, Atlan employs granular access policies that can be defined for both users and groups, controlling actions a user can take against various assets, from entire databases down to individual columns. This allows organizations to restrict access to sensitive data, ensuring that data governance policies align with organizational goals and compliance requirements (Source: https://docs.atlan.com/platform/concepts/authentication-and-authorization).","['Identity and access management\u200b\nFor centralized management of groups and users, Atlan uses granular access policies.\nAdmins can define policies to control both which actions a user can take and against which assets. These can be as broad as entire databases down to individual columns. Organizations can even build policies based on asset classification. This opens up the ability to restrict access to sensitive data like Personally Identifiable Information (PII)   -  an essential feature in the GDPR era.\nAtlan denies access by default, and explicit denials override any grants. You can even deny admin users access to assets, if you want.\nRoles\u200b\nYou must assign every user in Atlan a user role. These control basic levels of access.\nGroups\u200b\nYou can also add users to groups. Groups provide a more maintainable mechanism for applying access controls.\nPolicies\u200b\nYou can define access policies for both users and groups.', 'An organization may have data that can be publicly available, secure and confidential, or a combination of both. Atlan can help you ensure that all your data assets are governed by data governance policies and used in compliance with applicable laws and regulations.\nThe policy center in Atlan helps you build policies to align with your organizational goals for securing and managing data. You can currently choose from six different types of policies:', ""Tags | Atlan Documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlanTagsGet StartedTag ManagementConceptsWhat are tags?Build governanceTagsConceptsWhat are tags?TagsAtlan allows admin users to create tags for classifying data assets. You can use these tags in several ways:\n\nTo identify important characteristics of data assets\nFor grouping different assets together\nTo apply granular access policies against those groupings\n\nDid you know?As the volume of your data and its consumption increases, tags help you maintain data security.\nFor example, you can use tags to identify assets according to:\n\nData protection guidelines such as GDPR and CCPA compliance,\xa0for example:\n\nRight to be forgotten (RTBF) as a tag for GDPR compliance\nRight to know or right to delete for CCPA regulations\nDrive data cleanup for user data management at source"", 'Infrastructure security: Restrict network access to the control planes as well as nodes.\nAccess policies: Administrators can restrict user access to certain assets.\nBring your own credentials (BYOC): Users can provide their own data store credentials to query data.\n\nWhat data is Atlan actually bringing in?\u200b\nAtlan enables you to search and discover metadata, not the data itself.\nAs a data catalog of all your data assets, Atlan enables you to:\n\nExtract metadata from source systems via pushdown queries or API requests.\nProcess data with the sample data and query features, both of which can be turned off.\nPush down queries when sample data or query functionality is used, so, the results are neither cached nor stored in Atlan.\nIntegrate with your supported data sources via a service account with read-only permissions to the data source and complete control over these permissions.']"
How does Atlan handle data and metadata persistence?,"Atlan is a virtualized solution that does not move or store your data. It crawls metadata from upstream sources and stores that metadata in a secure VPC, while data queries are pushed directly to the existing storage layers.","Atlan handles data and metadata persistence by processing metadata that reaches its SaaS tenant and storing it in the Atlan metastore, making it searchable and governable. Importantly, Atlan does not move or store the actual data from existing storage layers; instead, it crawls metadata from upstream data sources and stores this metadata in a secure virtual private cloud (VPC). When queries are made, Atlan pushes them to existing processing layers, such as databases or data warehouses, ensuring that the original data remains in place (Source: https://docs.atlan.com/platform/concepts/data-and-metadata-persistence).","['Data and metadata persistence | Atlan Documentation', 'Processing and persistence in Atlan: Once the metadata reaches your Atlan SaaS tenant, it gets processed by Atlan workflows and persisted in the Atlan metastore where it becomes searchable and governable.', ""Skip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlanGet StartedWhat is Atlan?Quick Start GuidesCore ConceptsAuthentication and authorizationData and metadata persistenceEncryption and key managementHigh availability and disaster recovery (HA/DR)AdministrationSecurity & ComplianceReferencesFAQsGet StartedCore ConceptsData and metadata persistenceOn this pageData and metadata persistenceAtlan is a fully virtualized solution that does not involve moving data from existing storage layers. Atlan crawls metadata from upstream data sources and stores it in a secure VPC (virtual private cloud).\nAtlan pushes any queries to existing processing layers. For example, directly to your database, warehouse, or a processing layer such as Athena or Presto on top of blob storage. So the data itself stays put   -  Atlan does not move or store it."", 'Custom Metadata | Atlan Documentation']"
Where does Atlan store asset metadata and user data?,"Atlan stores asset metadata in Apache Atlas, Elasticsearch, and Cassandra. It stores user data, including roles and groups, in its own PostgreSQL database.","Atlan stores asset metadata in several locations: 
- **Apache Atlas**, which is a graph database layer that stores entity relationships and attributes; 
- **Elasticsearch**, which optimizes search capabilities within the product; and 
- **Cassandra**, which serves as the persistence back-end for this metadata (Source: https://docs.atlan.com/platform/concepts/data-and-metadata-persistence).  

For user data, Atlan utilizes its own **PostgreSQL database** to store information about users, roles, and groups. Additionally, **Keycloak** leverages this data for access and identity management. Notably, Atlan hashes all sensitive information, such as passwords, ensuring secure storage, and any user data transmitted online is protected via SSL encryption over HTTPS (Source: https://docs.atlan.com/platform/concepts/data-and-metadata-persistence).","['asset metadata\nuser data\n\nAsset metadata\u200b\nAtlan stores asset metadata, including lineage, in:\n\nApache Atlas, a graph database layer that stores entity relationships and attributes\nElasticsearch, to optimize search on the product\nCassandra, as the persistence back-end\n\nUser data\u200b\nAtlan stores data on users, roles, and groups in its own PostgreSQL database. Keycloak uses this information for access and identity management.\nAtlan hashes all sensitive fields like passwords and stores them securely. Any user data transmitted over the internet is SSL-encrypted over HTTPS.Tags:upstream-dependenciesdata-sourcesPreviousAuthentication and authorizationNextEncryption and key managementData previews and queriesMetadata storageCopyright © 2025 Atlan Pte. Ltd.', 'Data and metadata persistence | Atlan Documentation', 'Supported asset metadata\u200b\nAtlan maps the following asset metadata properties to it contract properties:', 'Processing and persistence in Atlan: Once the metadata reaches your Atlan SaaS tenant, it gets processed by Atlan workflows and persisted in the Atlan metastore where it becomes searchable and governable.']"
"What permissions are required when generating a token for crawling dbt in Atlan, and how do they differ between dbt Cloud Team and Enterprise plans?","When generating a Service Account Token, Team plans require Read-only access to all projects, while Enterprise plans require Job Viewer access. For a Personal Access Token, the user creating it must have Job Viewer access to all relevant projects.","When generating a token for crawling dbt in Atlan, the required permissions differ based on the dbt Cloud plan:

1. **For Team plans**: You need to add **Read-only access** to all projects you want to integrate into Atlan. This permission is essential for authorizing requests to both the dbt Cloud Administrative API and dbt Cloud Discovery API (Source: https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/set-up-dbt-cloud).

2. **For Enterprise plans**: You must add **Job Viewer access** to all projects you want to integrate. This permission provides read-only access to your dbt account, project, environment, job, and run metadata (Source: https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/crawl-dbt).","['Create a token\u200b\nBe sure to copy the generated token for crawling dbt.\nService account token\u200b\nOnly dbt Cloud administrators can generate service account tokens. This is required for authenticating as a service account user and to set up granular access permissions.\nTo generate a service account token, follow the steps in dbt documentation and configure the following permissions:', ""Skip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlandbtGet StartedSet up dbt CloudSet up dbt CoreCrawl dbt AssetsManage dbt in AtlanImpact AnalysisReferencesTroubleshootingConnect dataETL ToolsdbtGet StartedSet up dbt CloudOn this pageSet up dbt CloudWho can do this?You will probably need your dbt Cloud administrator to complete these steps   -  you may not have access yourself.\nIf you have a dbt Cloud account, Atlan can help enrich your assets with dbt metadata.\ndangerTo enable Atlan to fetch metadata for dbt models defined in your project, you must add the dbt docs generate command to the list of commands in the job run steps. This will produce a catalog.json file containing all the relevant metadata. Alternatively, you can select the Generate docs on run checkbox to automatically generate updated project docs each time a job runs. Refer to dbt documentation to learn more.\nCreate a token\u200b"", 'Team plans: add Read-only access to all projects you want to integrate into Atlan. This permission is required to authorize requests to both the dbt Cloud Administrative API and dbt Cloud Discovery API.\nEnterprise plans: add Job Viewer access to all projects you want to integrate into Atlan. This will provide read-only access to your dbt account, project, environment, job, and run metadata. Learn more about dbt Cloud Enterprise permissions.', ""Crawl dbt | Atlan Documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlandbtGet StartedCrawl dbt AssetsCrawl dbtManage dbt in AtlanImpact AnalysisReferencesTroubleshootingConnect dataETL ToolsdbtCrawl dbt AssetsCrawl dbtOn this pageCrawl dbt\nOnce you have configured dbt Cloud service token or uploaded your dbt Core project files to cloud storage, you can crawl dbt metadata into Atlan.\nTo enrich metadata in Atlan from dbt, review the order of operations and then complete the following steps.\nSelect the source\u200b\nTo select dbt as your source:\n\nIn the top right of any screen, navigate to New and then click New Workflow.\nFrom the list of packages, select dbt Assets and then click Setup Workflow.\n\nProvide your credentials\u200b\ndbt core\u200b\nTo enter your dbt Core credentials:""]"
What is the warning mentioned in the text regarding dbt crawler workflows in Atlan?,"The text warns that User API tokens will be deprecated by October 22, 2024. If you have existing dbt crawler workflows in Atlan using these tokens, you must modify their configuration with updated credentials, such as a service account or a personal access token, to avoid encountering errors.","The warning mentioned regarding dbt crawler workflows in Atlan is that when configuring the crawler for extracting dbt assets directly from the dbt files location, you must provide specific information, including the S3 bucket containing the dbt Core files, the prefix within the S3 bucket where the files are located, and the S3 region where the bucket is located. This is crucial to ensure that the crawler can properly access and extract the required assets (Source: https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/crawl-dbt).","['References\u200b\n\nWhat does Atlan crawl from dbt Core: Learn about the dbt Core assets and metadata that Atlan discovers and catalogs.\nPreflight checks for dbt: Verify prerequisites before setting up the dbt Core connector.\n\nTroubleshooting\u200b\n\nTroubleshooting connectivity: Resolve common dbt Core connection issues and errors.\n\n\ndbt cloud\u200b\nGet started\u200b\n\nSet up the connector\nCrawl dbt Cloud assets\n\nGuides\u200b\n\nManage dbt tags: Manage and sync tags from dbt Cloud.\nAdd impact analysis in GitHub: Enable impact analysis for dbt Cloud projects in GitHub.\nAdd impact analysis in GitLab: Enable impact analysis for dbt Cloud projects in GitLab.\nMigrate from dbt to Atlan Action: Migrate your dbt Cloud integration to Atlan Action.\nEnrich Atlan through dbt: Update and enrich Atlan metadata using dbt Cloud.\n\nReferences\u200b', ""What does Atlan crawl from dbt Cloud? | Atlan Documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlandbtGet StartedCrawl dbt AssetsManage dbt in AtlanImpact AnalysisReferencesWhat does Atlan crawl from dbt Cloud?What does Atlan crawl from dbt Core?TroubleshootingConnect dataETL ToolsdbtReferencesWhat does Atlan crawl from dbt Cloud?On this pageWhat does Atlan crawl from dbt Cloud?Atlan crawls and maps the following assets and properties from dbt Cloud. Atlan also supports lineage between the following:\n\ndbt models\ndbt seeds\ndbt sources\nSQL tables and views materialized by dbt models, dbt seeds, dbt sources\nColumn-level lineage for these entities"", ""Crawl dbt | Atlan Documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlandbtGet StartedCrawl dbt AssetsCrawl dbtManage dbt in AtlanImpact AnalysisReferencesTroubleshootingConnect dataETL ToolsdbtCrawl dbt AssetsCrawl dbtOn this pageCrawl dbt\nOnce you have configured dbt Cloud service token or uploaded your dbt Core project files to cloud storage, you can crawl dbt metadata into Atlan.\nTo enrich metadata in Atlan from dbt, review the order of operations and then complete the following steps.\nSelect the source\u200b\nTo select dbt as your source:\n\nIn the top right of any screen, navigate to New and then click New Workflow.\nFrom the list of packages, select dbt Assets and then click Setup Workflow.\n\nProvide your credentials\u200b\ndbt core\u200b\nTo enter your dbt Core credentials:"", 'The DbtCrawler package will create a workflow to crawl assets from dbt files location.\nYou must provide Atlan client.\nYou must provide a name for the connection that the dbt assets will exist within\n\nYou must specify at least one connection admin, either:\n\neveryone in a role (in this example, all $admin users)\na list of groups (names) that will be connection admins.\na list of users (names) that will be connection admins.\n\n\n\nTo configure the crawler for extracting dbt assets directly from the dbt files location, you must provide the following information:\n\ns3 bucket containing the dbt Core files.\nprefix within the S3 bucket where the dbt Core files are located.\ns3 region where the bucket is located.\n\n\n\nYou can also optionally specify the qualifiedName of a connection to a source (such as Snowflake), to limit the crawling of dbt assets to that existing connection in Atlan.']"
What is Atlan's process for disaster recovery if a tenant needs to be recreated?,"In case of a disaster, Atlan automates the recreation of a tenant by onboarding a new one, restoring data from the last backup using Argo Workflows, scaling down the old tenant, updating the domain and Cloudflare records, with the entire process taking around 3-4 hours.","Atlan's disaster recovery process involves several automated actions to restore a tenant in the event of a disaster. When a disaster is detected, the Disaster Assessment Team is notified and evaluates the situation to prioritize remediation based on critical services. To restore a tenant, Atlan performs the following actions:

1. **Recreation of the Tenant**: The tenant is recreated as part of the recovery actions.
2. **Automated Backup Restoration**: All components of the tenant are restored from backup, which typically takes around 3-4 hours. If there is data loss for any specific component, recovery can be done from the last backup (Source: https://docs.atlan.com/platform/concepts/high-availability-and-disaster-recovery-ha-dr).  
3. **Post-Recovery Validation**: After restoration, Atlan conducts data integrity checks and system tests to ensure that the restored data is accurate and that all components are functioning correctly (Source: https://docs.atlan.com/platform/concepts/high-availability-and-disaster-recovery-ha-dr).","['Disaster recovery (DR)\u200b\nAtlan follows industry best practices for disaster recovery. Atlan uses Argo Workflows for orchestration to successfully implement a disaster recovery strategy and reduce production downtime, so that business impact is minimized in the event of an outage.\nIf a disaster is detected, the Disaster Assessment Team   -  comprising key stakeholders from IT, platform, operations, and support   -  will be promptly notified through Atlan’s established communication channels. The team will conduct a thorough evaluation to determine the extent of the damage and prioritize remediation based on an internal list of critical services and applications.\nIn case of a disaster, a tenant will be recreated, and the following actions performed to restore the tenant:', 'All the aforementioned action items are automated. The entire process of restoring all the components of a tenant from backup takes around 3-4 hours. In case of data loss for any particular component, it can also be recovered from the last backup.\nHere are a few parameters that help reduce downtime and expedite the process of disaster recovery:\nInfrastructure\u200b\nSingle-tenant SaaS is the default deployment option for most Atlan users. In this model, Atlan manages the infrastructure needs and ensures that all instances are spread across multiple Availability Zones (AZ) in each AWS Region where the user instance is deployed.\nAvailability Zones are multiple, isolated locations within a single AWS Region. Multi-AZ deployments provide enhanced availability for instances within a single AWS Region. With multi-AZ, your data is synchronously replicated to standby in a different Availability Zone.\ndangerAtlan currently does not support multi-region deployment.\nAtlan service overview\u200b', 'Atlan can restore a single component in case of data corruption for any single point of failure, such as a metastore and its components like Elasticsearch and Cassandra. It is also possible to do a full-cluster restore in case of an unintended operation or a data loss or corruption event.\nDid you know?Argo Workflows powers all the backup and restore packages in Atlan. It includes a retry mechanism in case of any errors while completing the steps in the workflow. It also sends alerts in case of entire package failure as part of observability.\nMigration\u200b\nAtlan has an easy process to migrate the application to other AWS Regions. In case of total region outage and the need for migrating an instance to another region or account, this migration activity will be performed via Atlan’s backup and restore packages.\nRTO, RPO, and retention\u200b', 'Atlan carries out a daily backup of all critical services once every 24 hours, so in a worst case scenario provides an RPO of 24 hours.\nFor all critical applications, RTO is less than 3 hours.\nAtlan retains daily backups for 15 days.\n\nPost-recovery validation\u200b\nThe following post-recovery actions are performed:\n\nPost restoration, Atlan conducts data integrity checks to ensure that the restored data is accurate and complete.\nAtlan performs system tests to confirm that all components of the tenant are functioning correctly after restoration.\nTags:lineagedata-lineageimpact-analysisPreviousEncryption and key managementNextCloud logging and monitoringHigh availability (HA)Disaster recovery (DR)RTO, RPO, and retentionPost-recovery validationCopyright © 2025 Atlan Pte. Ltd.']"
How does Atlan manage backups for its tenants?,"Atlan performs an automated, full backup of each tenant daily, storing it encrypted in cloud storage with a retention period of 15 days. It backs up components like Elasticsearch, Cassandra, and Postgres, and monitors for any failures.","Atlan manages backups for its tenants through an automated daily process that runs at 3:00 AM UTC by default, although this schedule can be configured as needed by each organization. Each tenant's backup is stored in its respective cloud storage and is encrypted at rest using the Advanced Encryption Standard (AES) 256 algorithm, with the encryption key being rotated by the cloud provider. Access to the backup storage is controlled by Atlan and is only provided for troubleshooting purposes (Source: https://docs.atlan.com/platform/concepts/high-availability-and-disaster-recovery-ha-dr).

The backup process captures a full backup of all data—incremental backups are not performed. Atlan monitors the backup process to ensure that backups are not skipped and generates alerts for the support team if a backup fails. The entire restoration process from a backup takes approximately 3-4 hours, and in case of data loss for a specific component, recovery can be done from the last backup (Source: https://docs.atlan.com/platform/concepts/high-availability-and-disaster-recovery-ha-dr).

Atlan retains these daily backups for 15 days, providing a Recovery Point Objective (RPO) of 24 hours and a Recovery Time Objective (RTO) of less than 3 hours for critical applications (Source: https://docs.atlan.com/platform/concepts/high-availability-and-disaster-recovery-ha-dr). After restoration, Atlan performs data integrity checks and system tests to ensure the accuracy and functionality of the restored data (Source: https://docs.atlan.com/platform/concepts/high-availability-and-disaster-recovery-ha-dr).","['Backups and restore\u200b\nAtlan runs an automated daily backup of each tenant. By default, the backup is scheduled at 3:00 AM UTC, configurable as per the requirement of an organization.\nThe backup of each tenant is stored in its respective cloud storage. The backups are encrypted at rest by the default cloud provider key. This key uses the Advanced Encryption Standard (AES) 256 algorithm. Since Atlan uses the cloud provider key, the key is rotated by the cloud provider.\nAtlan controls access to the cloud storage where the backup is stored, and only provides access in case of troubleshooting an issue. Each backup process captures a full backup of all the data, with no incremental backups being performed. Atlan also monitors the backup to ensure that backups are not skipped. Alerts are generated in case a backup run fails for the support team to examine the issue.', 'All the aforementioned action items are automated. The entire process of restoring all the components of a tenant from backup takes around 3-4 hours. In case of data loss for any particular component, it can also be recovered from the last backup.\nHere are a few parameters that help reduce downtime and expedite the process of disaster recovery:\nInfrastructure\u200b\nSingle-tenant SaaS is the default deployment option for most Atlan users. In this model, Atlan manages the infrastructure needs and ensures that all instances are spread across multiple Availability Zones (AZ) in each AWS Region where the user instance is deployed.\nAvailability Zones are multiple, isolated locations within a single AWS Region. Multi-AZ deployments provide enhanced availability for instances within a single AWS Region. With multi-AZ, your data is synchronously replicated to standby in a different Availability Zone.\ndangerAtlan currently does not support multi-region deployment.\nAtlan service overview\u200b', 'Atlan carries out a daily backup of all critical services once every 24 hours, so in a worst case scenario provides an RPO of 24 hours.\nFor all critical applications, RTO is less than 3 hours.\nAtlan retains daily backups for 15 days.\n\nPost-recovery validation\u200b\nThe following post-recovery actions are performed:\n\nPost restoration, Atlan conducts data integrity checks to ensure that the restored data is accurate and complete.\nAtlan performs system tests to confirm that all components of the tenant are functioning correctly after restoration.\nTags:lineagedata-lineageimpact-analysisPreviousEncryption and key managementNextCloud logging and monitoringHigh availability (HA)Disaster recovery (DR)RTO, RPO, and retentionPost-recovery validationCopyright © 2025 Atlan Pte. Ltd.', 'The lifecycle policy for backups in the cloud provider is set to 15 days, which means Atlan will retain backups for all the components for 15 days.\nBackups of the following components are taken on a daily basis:']"
What are the Recovery Time Objective (RTO) and Recovery Point Objective (RPO) for Atlan?,"For all critical applications, Atlan's RTO is less than 3 hours. Since backups are performed daily, its RPO is 24 hours in a worst-case scenario.","Atlan has defined its Recovery Time Objective (RTO) as less than 3 hours for all critical applications, meaning that in the event of a disruption, services can be restored within this time frame. The Recovery Point Objective (RPO) is set at 24 hours, which indicates that in a worst-case scenario, the maximum allowable data loss is one day's worth of data, as daily backups are conducted every 24 hours (Source: https://docs.atlan.com/platform/concepts/high-availability-and-disaster-recovery-ha-dr).","['Atlan carries out a daily backup of all critical services once every 24 hours, so in a worst case scenario provides an RPO of 24 hours.\nFor all critical applications, RTO is less than 3 hours.\nAtlan retains daily backups for 15 days.\n\nPost-recovery validation\u200b\nThe following post-recovery actions are performed:\n\nPost restoration, Atlan conducts data integrity checks to ensure that the restored data is accurate and complete.\nAtlan performs system tests to confirm that all components of the tenant are functioning correctly after restoration.\nTags:lineagedata-lineageimpact-analysisPreviousEncryption and key managementNextCloud logging and monitoringHigh availability (HA)Disaster recovery (DR)RTO, RPO, and retentionPost-recovery validationCopyright © 2025 Atlan Pte. Ltd.', 'RTO, RPO, and retention\u200b\nGreater RTOs and RPOs as well as system recovery are crucial for ensuring that multiple mission-critical applications are quickly restored. It is now possible to minimize the impact of a disruption and perform a recovery within a few hours of an outage.', 'Atlan can restore a single component in case of data corruption for any single point of failure, such as a metastore and its components like Elasticsearch and Cassandra. It is also possible to do a full-cluster restore in case of an unintended operation or a data loss or corruption event.\nDid you know?Argo Workflows powers all the backup and restore packages in Atlan. It includes a retry mechanism in case of any errors while completing the steps in the workflow. It also sends alerts in case of entire package failure as part of observability.\nMigration\u200b\nAtlan has an easy process to migrate the application to other AWS Regions. In case of total region outage and the need for migrating an instance to another region or account, this migration activity will be performed via Atlan’s backup and restore packages.\nRTO, RPO, and retention\u200b', 'Disaster recovery (DR)\u200b\nAtlan follows industry best practices for disaster recovery. Atlan uses Argo Workflows for orchestration to successfully implement a disaster recovery strategy and reduce production downtime, so that business impact is minimized in the event of an outage.\nIf a disaster is detected, the Disaster Assessment Team   -  comprising key stakeholders from IT, platform, operations, and support   -  will be promptly notified through Atlan’s established communication channels. The team will conduct a thorough evaluation to determine the extent of the damage and prioritize remediation based on an internal list of critical services and applications.\nIn case of a disaster, a tenant will be recreated, and the following actions performed to restore the tenant:']"
What is the specific condition that dbt assets must meet to be crawled by Atlan?,"A dbt asset must be in the 'applied' or 'built' state, which means it must have been part of a successful run. Assets that are only defined in project files but have not been executed will not be included.","For dbt assets to be crawled by Atlan, they must be part of the following categories: dbt models, dbt seeds, dbt sources, and SQL tables and views materialized by these entities. Additionally, Atlan supports column-level lineage for these assets (Source: https://docs.atlan.com/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-core).","[""What does Atlan crawl from dbt Core? | Atlan Documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlandbtGet StartedCrawl dbt AssetsManage dbt in AtlanImpact AnalysisReferencesWhat does Atlan crawl from dbt Cloud?What does Atlan crawl from dbt Core?TroubleshootingConnect dataETL ToolsdbtReferencesWhat does Atlan crawl from dbt Core?On this pageWhat does Atlan crawl from dbt Core?Atlan crawls and maps the following assets and properties from dbt Core. Atlan also supports lineage between the following:\n\ndbt models\ndbt seeds\ndbt sources\nSQL tables and views materialized by dbt models, dbt seeds, dbt sources\nColumn-level lineage for these entities\n\nOnce you've crawled dbt, you can use dbt-specific filters for quick asset discovery:"", ""What does Atlan crawl from dbt Cloud? | Atlan Documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlandbtGet StartedCrawl dbt AssetsManage dbt in AtlanImpact AnalysisReferencesWhat does Atlan crawl from dbt Cloud?What does Atlan crawl from dbt Core?TroubleshootingConnect dataETL ToolsdbtReferencesWhat does Atlan crawl from dbt Cloud?On this pageWhat does Atlan crawl from dbt Cloud?Atlan crawls and maps the following assets and properties from dbt Cloud. Atlan also supports lineage between the following:\n\ndbt models\ndbt seeds\ndbt sources\nSQL tables and views materialized by dbt models, dbt seeds, dbt sources\nColumn-level lineage for these entities"", 'The DbtCrawler package will create a workflow to crawl assets from dbt cloud.\nYou must provide Atlan client.\nYou must provide a name for the connection that the dbt assets will exist within.\n\nYou must specify at least one connection admin, either:\n\neveryone in a role (in this example, all $admin users)\na list of groups (names) that will be connection admins.\na list of users (names) that will be connection admins.\n\n\n\nTo configure the crawler for extracting dbt assets directly from dbt cloud account then you must provide the following information:\n\nhostname of your dbt cloud instance.\ntoken to use to authenticate against dbt cloud instance.\nwhether to use a multi-tenant cloud config (true), otherwise a single-tenant cloud config (false).\n\n\n\nYou can also optionally specify the qualifiedName of a connection to a source (such as Snowflake), to limit the crawling of dbt assets to that existing connection in Atlan.', ""Crawl dbt | Atlan Documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlandbtGet StartedCrawl dbt AssetsCrawl dbtManage dbt in AtlanImpact AnalysisReferencesTroubleshootingConnect dataETL ToolsdbtCrawl dbt AssetsCrawl dbtOn this pageCrawl dbt\nOnce you have configured dbt Cloud service token or uploaded your dbt Core project files to cloud storage, you can crawl dbt metadata into Atlan.\nTo enrich metadata in Atlan from dbt, review the order of operations and then complete the following steps.\nSelect the source\u200b\nTo select dbt as your source:\n\nIn the top right of any screen, navigate to New and then click New Workflow.\nFrom the list of packages, select dbt Assets and then click Setup Workflow.\n\nProvide your credentials\u200b\ndbt core\u200b\nTo enter your dbt Core credentials:""]"
"After crawling dbt, what are some of the dbt-specific filters and custom metadata Atlan uses to enrich assets?","Atlan enriches dbt assets by providing dbt-specific filters for discovery, such as by 'Test status', 'Job status', and 'Project name'. It also populates custom metadata, syncing dbt tags and mapping properties like 'alias' and 'uniqueId' to the assets in Atlan.","Atlan enriches assets after crawling dbt by populating custom metadata, including tags and specific properties. Some of the dbt-specific filters and custom metadata that Atlan utilizes include:

1. **dbt Tags**: Atlan allows syncing dbt tags with Atlan's assets, enabling users to update their dbt assets with these synced tags (Source: https://docs.atlan.com/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-core).

2. **Meta Field**: Atlan ingests information from dbt's meta field to enrich assets. This includes documenting attributes such as `certificateStatus` and `ownerUsers`, which can be specified in the dbt models (Source: https://docs.atlan.com/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-cloud).

3. **Custom Attributes**: The example provided in the context shows that custom attributes can be defined within the dbt model's meta section, allowing for detailed tracking and classification of assets (Source: https://developer.atlan.com/sdks/dbt).

Overall, these functionalities help maintain up-to-date documentation and enrich the assets managed within Atlan.","[""Atlan's dbt crawler also populates custom metadata to further enrich the assets in Atlan. The Atlan dbt-specific property column in the tables below gives the name of the mapped custom metadata property in Atlan.\nDid you know?Atlan lets you sync your dbt tags and update your dbt assets with the synced tags. You can also map other metadata to Atlan's assets through your dbt models.\nTables\u200b\nAtlan maps tables from dbt Core to its Table asset type."", ""Atlan's dbt connectivity also populates custom metadata to further enrich the assets in Atlan. The Atlan dbt-specific property column in the tables below gives the name of the mapped custom metadata property in Atlan.\nDid you know?Atlan enables you to sync your dbt tags and update your dbt assets with the synced tags. It's also possible to map other metadata on Atlan's assets through your dbt models.\nTables\u200b\nAtlan maps tables from dbt Cloud to its Table asset type."", 'You can use dbt\'s meta field  to enrich metadata resources from dbt into Atlan. Atlan will ingest the information from this field and update the assets in Atlan accordingly.\nWith this, you have a powerful way to keep the dbt assets documented directly as part of your dbt work.\nThe following is an example:\ndbt example 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37version: 2\nmodels:\n  - name: customers\n    description: >- # (1)\n      This table has basic information about a customer, as well as some derived\n      facts based on a customer\'s orders.\n    meta: # (2)\n      atlan: # (3)\n        attributes: # (4)\n          certificateStatus: DRAFT\n          ownerUsers: [""bryan"", ""ashwin""]\n        classifications: # (5)\n          - typeName: ""ipubxAPPb0zRcNU1Gkjs9b""\n            propagate: true\n            removePropagationsOnEntityDelete: true\n            restrictPropagationThroughLineage: true', ""What does Atlan crawl from dbt Core? | Atlan Documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlandbtGet StartedCrawl dbt AssetsManage dbt in AtlanImpact AnalysisReferencesWhat does Atlan crawl from dbt Cloud?What does Atlan crawl from dbt Core?TroubleshootingConnect dataETL ToolsdbtReferencesWhat does Atlan crawl from dbt Core?On this pageWhat does Atlan crawl from dbt Core?Atlan crawls and maps the following assets and properties from dbt Core. Atlan also supports lineage between the following:\n\ndbt models\ndbt seeds\ndbt sources\nSQL tables and views materialized by dbt models, dbt seeds, dbt sources\nColumn-level lineage for these entities\n\nOnce you've crawled dbt, you can use dbt-specific filters for quick asset discovery:""]"
"How does Atlan handle data when a user runs a query, and is the data stored in Atlan?","Atlan does not store or cache data. When a query is run, it pushes a SQL query directly to the underlying source database. The results are encrypted in transit, and Atlan applies access policies before displaying them.","Atlan handles data by pushing all queries directly to the underlying source database, meaning that it does not store or cache any data within the platform. Each time a user runs a query, Atlan streams the results in real-time from the data source, ensuring that the displayed data is always up to date and accurate (Source: https://docs.atlan.com/platform/concepts/data-and-metadata-persistence). Atlan does not move or store the actual data; it operates by accessing the data where it resides (Source: https://docs.atlan.com/faq/basic-platform-usage). 

In terms of storage, Atlan only stores the metadata it collects and creates, which is kept within a secure VPC (virtual private cloud). This metadata includes information about data assets but not the actual data itself (Source: https://docs.atlan.com/platform/concepts/data-and-metadata-persistence). Therefore, Atlan serves primarily as a metadata catalog rather than a data storage solution (Source: https://docs.atlan.com/platform/concepts/data-and-metadata-persistence).","[""How does Atlan access the data for queries?\u200b\nAtlan pushes all queries to the source. Each query generated in Atlan is a pushdown SQL query into the underlying source database. This data is neither stored nor cached anywhere in Atlan. It's encrypted in transit using AWS Key Management Service (AWS KMS). See also Encryption and key management.\nIn addition, Atlan applies access policies to the results before displaying them.\nWhy replace my current IDE with Atlan?\u200b\nAtlan's Insights feature is far and beyond any other IDE:"", ""Skip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlanGet StartedWhat is Atlan?Quick Start GuidesCore ConceptsAuthentication and authorizationData and metadata persistenceEncryption and key managementHigh availability and disaster recovery (HA/DR)AdministrationSecurity & ComplianceReferencesFAQsGet StartedCore ConceptsData and metadata persistenceOn this pageData and metadata persistenceAtlan is a fully virtualized solution that does not involve moving data from existing storage layers. Atlan crawls metadata from upstream data sources and stores it in a secure VPC (virtual private cloud).\nAtlan pushes any queries to existing processing layers. For example, directly to your database, warehouse, or a processing layer such as Athena or Presto on top of blob storage. So the data itself stays put   -  Atlan does not move or store it."", ""Not sure on the difference between data and metadata? Try our helpful primer.\nData previews and queries\u200b\nAtlan gives users the ability to see sample data previews for a data asset as well as the results for any queries run on Atlan.\nIn both cases, Atlan pushes the request upstream to the data source, and shows a 100-row sample of the result to Atlan users. Atlan does not cache any of this data. So each time a user previews or queries data, it is re-queried from the source.\nEvery time a user runs a query, Atlan streams query results in batches directly from your data source. Since the data is streamed in real-time from the data source, there is no need to persist the query results in Atlan's cache or storage layer. This ensures that the data displayed is always up to date and accurate, eliminating the need for storing intermediate query results.\nMetadata storage\u200b\nAtlan stores the metadata it collects and creates in applications and databases within the VPC. This includes:"", 'Infrastructure security: Restrict network access to the control planes as well as nodes.\nAccess policies: Administrators can restrict user access to certain assets.\nBring your own credentials (BYOC): Users can provide their own data store credentials to query data.\n\nWhat data is Atlan actually bringing in?\u200b\nAtlan enables you to search and discover metadata, not the data itself.\nAs a data catalog of all your data assets, Atlan enables you to:\n\nExtract metadata from source systems via pushdown queries or API requests.\nProcess data with the sample data and query features, both of which can be turned off.\nPush down queries when sample data or query functionality is used, so, the results are neither cached nor stored in Atlan.\nIntegrate with your supported data sources via a service account with read-only permissions to the data source and complete control over these permissions.']"
"Is it possible to disable the data querying feature in Atlan for all users, and who can perform this action?","Yes, it is possible to block all users from querying data across all assets. An admin user can perform this action by toggling the 'Insights' option off from the admin workspace.","Yes, it is possible to disable the data querying feature in Atlan for all users. To do this, an admin user needs to follow these steps: 

1. From the left menu of any screen, click on **Admin**.
2. Under **Workspace**, click on **Labs**.
3. Toggle off the **Insights** option, which will deactivate all sub-options of Insights as well, effectively blocking all querying across all data assets (Source: https://docs.atlan.com/product/capabilities/governance/custom-metadata/how-tos/disable-data-access). 

Only admin users in Atlan have the permissions necessary to configure these options, including disabling data access (Source: https://docs.atlan.com/product/administration/labs/how-tos/disable-user-activity).","[""Disable data access | Atlan Documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlanCustom MetadataGet StartedAccess ManagementControl access to metadata and data?Disable data accessBadge ManagementStructure ManagementReferencesConceptsFAQBuild governanceCustom MetadataAccess ManagementDisable data accessOn this pageDisable data accessWho can do this?You will need to be an admin user in Atlan to configure these options.\nWhat if you want to block access to data for your users, and only allow them to access metadata?\nThere are different ways to do this in Atlan. From the most wide-reaching to the most granular:\nBlock all querying\u200b\nTo stop all users from querying data, across all data assets:\n\nFrom the left menu of any screen, click Admin.\nUnder Workspace, click Labs.\nToggle off the Insights option. (This should also deactivate all sub-options of Insights.)"", ""Disable user activity | Atlan Documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlanAdministrationGet StartedFeature ManagementDisable user activityHow to enable associated termsHow to enable discovery of process assetsHow to enable sample data downloadHow to enable scheduled queriesRestrict asset visibilityRestrict glossary visibilityMonitoringTemplatesConfigure AtlanAdministrationFeature ManagementDisable user activityOn this pageDisable user activity\nWho can do this?You will need to be an admin user in Atlan to disable user activity on asset profiles.\nYou can view recently visited users and total views on your assets in Atlan by default. To disable asset profile visitors, complete these steps.\nDisable asset profile visitors\u200b\nTo disable asset profile visitors for your assets in Atlan:"", ""Can we restrict who can query our data warehouse? | Atlan Documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlanInsightsGet StartedQuery ManagementCredentialsConceptsReferencesFAQAre there any limits on concurrent queries?Can I query any DW/DL?Can I turn off sample data preview for the entire organization?Can we restrict who can query our data warehouse?How can I identify an Insights query in my database access log?Monitor for runaway queries?How to use parameterized queries?What controls the frequency of queries?Why do I only see tables from the same schema to join from in a visual query?TroubleshootingUse dataInsightsFAQCan we restrict who can query our data warehouse?Can we restrict who can query our data warehouse?You can restrict who can query data at different levels of granularity:"", 'Block all querying\nBlock all querying of a source\nBlock specific users from querying specific assets by asset or tag\nTags:atlandocumentationfaq-insightsPreviousCan I turn off sample data preview for the entire organization?NextHow can I identify an Insights query in my database access log?Copyright © 2025 Atlan Pte. Ltd.']"
What are the steps and prerequisites for enabling Atlan event logs in an AWS S3 bucket?,"To enable event logs in AWS, the customer's S3 bucket must have versioning enabled, and they must provide their account ID, bucket name, and region. Atlan then provides a bucket policy for the customer to attach, after which Atlan support completes the final configuration.","To enable Atlan event logs in an AWS S3 bucket, follow these steps and prerequisites:

### Prerequisites
1. **Enable Bucket Versioning**: Both the source and destination S3 buckets must have versioning enabled. Refer to the AWS documentation for instructions on how to enable versioning.
2. **Provide Bucket Details**: You need to supply the following information to Atlan:
   - Account ID
   - Bucket name
   - Region
   Atlan will use these details to create an IAM role and provide you with a bucket policy that must be attached to your S3 bucket.

### Steps to Enable Event Logs
1. **Attach the Bucket Policy**: Once you receive the bucket policy from Atlan, attach it to your S3 bucket. This policy is crucial for allowing Atlan to access the logs.
2. **Confirm Policy Attachment**: After you have attached the bucket policy, confirm this with Atlan. This step ensures that Atlan can proceed with the final configuration.
3. **Final Configuration by Atlan**: Atlan support will complete the setup of log replication to your S3 bucket after the policy is confirmed to be attached.

### Setting Up the S3 Bucket
- Ensure that the query files are available in the S3 bucket. You can either upload these files to the Atlan deployment bucket or use your own S3 bucket.
- To avoid access issues, it is recommended to upload the required files to the same S3 bucket as Atlan. You may need to raise a support request to obtain the details of your Atlan bucket.
- Configure access by adding the following IAM policy to the default EC2 instance role used by the Atlan EKS cluster:
  ```json
  {
    ""Version"": ""2012-10-17"",
    ""Statement"": [
      {
        ""Sid"": ""VisualEditor0"",
        ""Effect"": ""Allow"",
        ""Action"": [
          ""s3:GetBucketLocation"",
          ""s3:ListBucket"",
          ""s3:GetObject""
        ],
        ""Resource"": [
          ""arn:aws:s3:::<bucket-name>"",
          ""arn:aws:s3:::<bucket-name>/<prefix>/*""
        ]
      }
    ]
  }
  ```

### Continuous Replication
- Application audit logs will be streamed to Atlan's S3 bucket in near real-time (within 10 seconds of generation). Once these logs are available in Atlan's bucket, they will be replicated to your organization's S3 bucket within 15 minutes, ensuring ongoing and timely access to the logs (Source: https://docs.atlan.com/platform/references/cloud-logging-and-monitoring).","['Secure delivery- Logs are encrypted in transit and at rest, with mechanisms to validate data integrity.\n\n\nCustomer access: Logs are easily accessible through S3 or GCS, allowing for a flexible monitoring and alerting setup.\n\n\nEnabling event logs in AWS\u200b\n\nPrerequisites\u200b\n\nEnable bucket versioning. Both source and destination buckets must have versioning enabled. See AWS documentation.\nCustomer-provided bucket details: account ID, bucket name, and region.\nAtlan will use these details to create an IAM role on the Atlan side and then provide you with the bucket policy to be attached.\nOnce you have confirmed that the bucket policy has been attached, Atlan will complete the final step of setting up log replication. Atlan support will complete the configuration on the Atlan side.', 'Set up the S3 bucket\u200b\nThe query files must be available in an S3 bucket. You can either upload these files to the Atlan deployment bucket or use your own S3 bucket.\nOption 1: Use the Atlan S3 bucket\u200b\nTo avoid access issues, we recommend uploading the required files to the same S3 bucket as Atlan. Raise a support request to get the details of your Atlan bucket and include the ARN value of the IAM user or IAM role we can provision access to.\nTo configure access, add the following IAM policy to the default EC2 instance role used by the Atlan EKS cluster.\n{  ""Version"": ""2012-10-17"",  ""Statement"": [    {      ""Sid"": ""VisualEditor0"",      ""Effect"": ""Allow"",      ""Action"": [        ""s3:GetBucketLocation"",        ""s3:ListBucket"",        ""s3:GetObject""      ],      ""Resource"": [        ""arn:aws:s3:::<bucket-name>"",        ""arn:aws:s3:::<bucket-name>/<prefix>/*""      ]    }  ]}', ""Set up Amazon S3 | Atlan Documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlanAmazon S3Get StartedSet up Amazon S3Inventory reportsCrawl S3 AssetsReferencesConnect dataStorageAmazon S3Get StartedSet up Amazon S3On this pageSet up Amazon S3\nThis guide walks you through creating IAM permissions and authentication credentials to allow Atlan to catalog your S3 buckets and objects.\nwarningThis integration catalogs only S3 buckets and objects. It doesn't support data lineage.\nPrerequisites\u200b\nBefore you begin:\n\nSet up S3 inventory reports, required only if you plan to use inventory-based ingestion.\n\nPermissions required\u200b\nTo complete this setup, you'll need:"", ""Continuous replication to S3 bucket\u200b\nApplication audit logs are streamed to Atlan's S3 bucket in near real time — within 10 seconds of being generated. This is a continuous process. Once the logs are available in Atlan's bucket, the logs will be replicated to your organization's S3 bucket within 15 minutes. The replication is ongoing and occurs without delays. This ensures that logs are continuously transferred as they are generated, with no waiting period between replications.\nEnabling event logs in GCP\u200b""]"
"How does Atlan enable event log syncing for Google Cloud Platform (GCP), and how does the timing differ for GCS?","For GCP, Atlan uses a Logs Router. The customer provides a destination, Atlan creates a sink and a service account, and the customer must then configure permissions for that account. Sinks to Google Cloud Storage (GCS) buckets are processed hourly, while other destination types are processed in real time.","Atlan enables event log syncing for Google Cloud Platform (GCP) through the use of Logs Router, which transfers logs from the GCS bucket of your Atlan tenant to a designated destination bucket of your choice. This setup requires the organization to provide details of the destination, which must be supported by the Logs Router. Atlan will then create a Log Router sink and furnish you with a service account. After configuring the necessary permissions for the service account as per Google documentation, the logs will start syncing to the chosen destination (Source: https://docs.atlan.com/platform/references/cloud-logging-and-monitoring). 

In terms of timing, the logs generated from GCP are streamed to Atlan's S3 bucket in near real-time, within 10 seconds of creation. Once available in Atlan's bucket, these logs are replicated to your organization's S3 bucket within 15 minutes, ensuring ongoing and timely log transfers with minimal delays (Source: https://docs.atlan.com/platform/references/cloud-logging-and-monitoring).","['For Google Cloud Platform (GCP), Atlan utilizes Logs Router to transfer logs from the GCS bucket of your Atlan tenant to a destination bucket of your choice. The destination must be supported by the Logs Router.\n\nThe organization must provide details of the destination where the logs should be synced. This destination must be supported by the Logs Router.\nAtlan will create a Log Router sink and provide you with a service account.\nDepending on the selected destination, you will need to configure the necessary permissions for the service account as outlined in Google documentation.\nOnce you have configured the permissions, the logs will begin syncing to your preferred destination.', ""Continuous replication to S3 bucket\u200b\nApplication audit logs are streamed to Atlan's S3 bucket in near real time — within 10 seconds of being generated. This is a continuous process. Once the logs are available in Atlan's bucket, the logs will be replicated to your organization's S3 bucket within 15 minutes. The replication is ongoing and occurs without delays. This ensures that logs are continuously transferred as they are generated, with no waiting period between replications.\nEnabling event logs in GCP\u200b"", '2 docs tagged with ""gcp"" | Atlan Documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to main contentSearchPartner with usWhat\'s newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure Atlan2 docs tagged with ""gcp""View all tagsGoogle Cloud StorageIntegrate, catalog, and govern Google Cloud Storage assets in Atlan.Set up Google Cloud StorageConfigure Google Cloud Storage for secure metadata ingestion with Atlan.Copyright © 2025 Atlan Pte. Ltd.', ""Cloud logging and monitoring | Atlan Documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlanGet StartedWhat is Atlan?Quick Start GuidesCore ConceptsAdministrationCloud logging and monitoringGenerate HAR files and console logsTenant access managementTenant logsTenant monitoringTenant offboardingSecurity & ComplianceReferencesFAQsGet StartedAdministrationCloud logging and monitoringOn this pageCloud logging and monitoringAtlan exports IAM service event logs in the OpenTelemetry Protocol (OTLP) specification and securely delivers them to the Amazon S3 or Google Cloud Storage (GCS) bucket of your organization. This enables you to monitor login events and integrate logs with security information and event management (SIEM) systems for real-time security monitoring and alerts.\nKey aspects\u200b""]"
What are the key prerequisites you need to set up before using the Atlan GitHub Action for impact analysis?,"You must create an Atlan API token and assign it a persona with the necessary metadata policies (for example, Read/Update on dbt assets and Read on downstream connections). You also need to grant Read and write permissions to the default GITHUB_TOKEN in your repository.","Before using the Atlan GitHub Action for impact analysis, you need to set up several key prerequisites:

1. **Atlan API Token**: Create an Atlan API token and assign a persona to it.

2. **Metadata Policy**: Add a metadata policy that provides the necessary permissions on assets for the Atlan action to function correctly. For instance:
   - Asset (e.g., a table): Read only
   - Any downstream connections (e.g., Microsoft Power BI): Read only

3. **GITHUB_TOKEN Permissions**: Configure the default `GITHUB_TOKEN` permissions in your repository to grant read and write access. This enables the Atlan action to add or update comments on pull requests.

4. **Repository Secrets**: Create the following repository secrets:
   - `ATLAN_INSTANCE_URL`: The URL of your Atlan instance.
   - `ATLAN_API_TOKEN`: The value of the API token you created.

These steps are essential to ensure that the Atlan action can effectively perform impact analysis within your GitHub workflows (Source: https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/add-impact-analysis-in-github).","['Add impact analysis in GitHub | Atlan Documentation', 'Before running the action, you will need to create an Atlan API token.\nYou will also need to assign a persona to the API token and add a metadata policy that provides the requisite permissions on assets for the Atlan action to work. For example, you can add the following permissions:\n\nAsset, such as a table   -  Read only\nAny downstream connections, such as Microsoft Power BI   -  Read only\n\n\nYou will need to configure the default GITHUB_TOKEN permissions. Grant Read and write permissions to the GITHUB_TOKEN in your repository to allow the atlan-action to seamlessly add or update comments on pull requests. Refer to GitHub documentation to learn more.\n\nConfigure the action\u200b\nTo set up the Atlan action in GitHub:\n\nCreate repository secrets in your repository:\n\nATLAN_INSTANCE_URL with the URL of your Atlan instance.\nATLAN_API_TOKEN with the value of the API token.\n\n\nAdd the GitHub Action to your workflow:', ""Skip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlanContractsGet StartedImpact AnalysisAdd contract impact analysis in GitHubBuild governanceContractsImpact AnalysisAdd contract impact analysis in GitHubOn this pageAdd contract impact analysis in GitHub Private Preview\nImpact analysis helps you identify how modifications to your data contracts might impact downstream processes, data quality, and overall business operations. This can help you analyze proposed changes and mitigate potential risks before implementation.\nIf you have ever changed a data contract only to find out later that it broke a downstream table or dashboard, Atlan provides a GitHub Action to help you out.\nThis action places Atlan's impact analysis right into your pull request. So, you can view the potential downstream impact of your changes before merging the pull request.\nPrerequisites\u200b"", ""Skip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlandbtGet StartedCrawl dbt AssetsManage dbt in AtlanImpact AnalysisAdd impact analysis in GitHubAdd impact analysis in GitLabReferencesTroubleshootingConnect dataETL ToolsdbtImpact AnalysisAdd impact analysis in GitHubOn this pageAdd impact analysis in GitHubdangerFor existing users, the dbt-action is no longer maintained and will be deprecated eventually. Atlan strongly recommends migrating to the atlan-action. Refer to How to migrate from dbt to Atlan action to learn more and complete the migration.\nIf you have ever changed a dbt model only to find out later that it broke a downstream table or dashboard,\xa0Atlan provides a GitHub Action to help you out.\nThis action places Atlan's impact analysis right into your pull request. So, you can view the potential downstream impact of your changes before merging the pull request.\nPrerequisites\u200b""]"
How can you resolve an issue where the Atlan GitHub action fetches a dbt model from an incorrect environment?,"To ensure the action fetches a model from the correct environment, you can use the `DBT_ENVIRONMENT_BRANCH_MAP` input in your workflow file to map the GitHub branch to a specific dbt environment, for example, mapping the 'main' branch to 'dbt-prod'.","To resolve the issue where the Atlan GitHub action fetches a dbt model from an incorrect environment, you should map the GitHub branch to the specific dbt environment using the `DBT_ENVIRONMENT_BRANCH_MAP` input. This mapping helps the Atlan action identify which exact environment to pull the model from, especially when there are multiple models with the same name across different environments.

You can provide the mapping in the following format:

```
branch name : dbt environment name
```

For example, you can set it up as follows:

```yaml
DBT_ENVIRONMENT_BRANCH_MAP: |
  main: dbt-prod
  beta: dbt-test
```

By doing so, the Atlan action will be able to correctly parse lineage for the specified environment tied to the branch (Source: https://docs.atlan.com/apps/connectors/etl-tools/dbt/how-tos/add-impact-analysis-in-github).","[""Inputs\u200b\nNameDescriptionRequiredGITHUB_TOKENFor writing comments on PRs to print downstream assetstrueATLAN_INSTANCE_URLFor making API requests to the user's tenanttrueATLAN_API_TOKENFor authenticating API requests to the user's tenanttrueDBT_ENVIRONMENT_BRANCH_MAPFor mapping the GitHub branch with a specific dbt environmentfalseIGNORE_MODEL_ALIAS_MATCHINGFor turning off matching aliases using this variablefalse\nTroubleshooting the action\u200b\nWhy does the action fetch a model from an incorrect environment?\u200b\nIf there are multiple dbt models with the same name but across different environments in your Atlan instance, the action may fetch an incorrect model. In order to ensure that the action fetches a model from the right environment, you can map the GitHub branch with a specific dbt environment. This will allow the Atlan GitHub action to parse lineage for that specific environment.\nFor example, you can provide the mapping in this format   -  branch name : dbt environment name"", ""Inputs\u200b\nNameDescriptionRequiredGITLAB_TOKENFor writing comments on PRs to print downstream assetstrueATLAN_INSTANCE_URLFor making API requests to the user's tenanttrueATLAN_API_TOKENFor authenticating API requests to the user's tenanttrueDBT_ENVIRONMENT_BRANCH_MAPFor mapping the GitLab branch with a specific dbt environmentfalseIGNORE_MODEL_ALIAS_MATCHINGFor turning off matching aliases using this variablefalse\nTroubleshooting the action\u200b\nWhy does the action fetch a model from an incorrect environment?\u200b\nIf there are multiple dbt models with the same name but across different environments in your Atlan instance, the action may fetch an incorrect model. In order to ensure that the action fetches a model from the right environment, you can map the GitLab branch with a specific dbt environment. This will allow the Atlan GitLab CI/CD pipeline to parse lineage for that specific environment.\nFor example, you can provide the mapping in this format   -  branch name : dbt environment name"", 'For example, you can provide the mapping in this format   -  branch name : dbt environment name\nstages:  - get-downstream-impactget-downstream-impact-open:  stage: get-downstream-impact  image: node:20  variables:+    DBT_ENVIRONMENT_BRANCH_MAP: |+      main: [Enter Your Branch name]  script:    - git clone   - branch v1 https://github.com/atlanhq/atlan-action.git    - cd atlan-action    - npm install    - npm run build    - node ./adapters/index.js  environment:    name: get-downstream-impact  rules:    - if: \'$CI_PIPELINE_SOURCE == ""merge_request_event""\'    - if: \'$CI_COMMIT_BRANCH && $CI_OPEN_MERGE_REQUESTS\'      when: never    - if: \'$CI_COMMIT_BRANCH\'\nWhy does the action fetch a model by its alias and not model name?\u200b\nBy default, the action checks if there is an alias defined for a dbt model in the code and looks for the relevant asset in Atlan using that alias. To turn off matching aliases for your dbt models, you can set the IGNORE_MODEL_ALIAS_MATCHING input to true.', 'For example, you can provide the mapping in this format   -  branch name : dbt environment name\njobs:  get-downstream-impact:    name: Get Downstream Assets    runs-on: ubuntu-latest    steps:      - name: Run Action        uses: atlanhq/atlan-action@v1        with:          GITHUB_TOKEN: ${{secrets.GITHUB_TOKEN}}          ATLAN_INSTANCE_URL: ${{secrets.ATLAN_INSTANCE_URL}}          ATLAN_API_TOKEN: ${{secrets.ATLAN_API_TOKEN}}+         DBT_ENVIRONMENT_BRANCH_MAP: |+           main: dbt-prod+           beta: dbt-test\nWhy does the action fetch a model by its alias and not model name?\u200b\nBy default, the action checks if there is an alias defined for a dbt model in the code and looks for the relevant asset in Atlan using that alias. To turn off matching aliases for your dbt models, you can set the IGNORE_MODEL_ALIAS_MATCHING input to true.\nFor example:']"
What are the main steps to generate and securely provide troubleshooting files to Atlan support using Google Chrome?,"First, open the Developer Tools, start recording in the 'Network' tab, and then replicate the issue. Next, save the console log from the 'Console' tab and the HAR file from the 'Network' tab. Finally, it is recommended to edit the HAR file in a text editor to remove any sensitive information like passwords before uploading both files.","To generate and securely provide troubleshooting files to Atlan support using Google Chrome, follow these main steps:

1. **Launch Google Chrome** and navigate to the relevant webpage.

2. In the upper-right corner, click the three vertical dots to open the Chrome menu.

3. Select **More Tools**, then click **Developer Tools**.

4. In the Developer Tools, go to the **Network** tab and set the filtering option to **Fetch/XHR**.

5. Click **Preserve log** under the Network tab to start recording the network log (a red circle will appear). If it is black, click it to turn it red.

6. Refresh the webpage to allow Chrome to record interactions between the browser and the website.

7. Once the page loads, click the circle slash icon next to the red circle to clear the logs.

8. Replicate the issue you experienced (for instance, perform the action that triggers the error) so that the error is captured in the console.

9. After replicating the issue, navigate to the **Console** tab, right-click in the console box, select **Save as...**, and name your file.

10. **Edit the HAR file** in a text editor to remove any sensitive information before sending it to Atlan support, as HAR files can contain sensitive data like passwords and private keys (Source: https://docs.atlan.com/platform/how-tos/generate-har-files-and-console-logs).","['Console or network logs frequently provide critical error details that are required to determine the underlying cause of the issue or bug that you are experiencing.\nHAR files include all the network traffic from when you started recording, including sensitive information like passwords and private keys. To avoid including such information in a HAR file, Atlan recommends using a text editor to manually edit the file and remove any sensitive content before sending it to Atlan support.\n\nGenerate in Google Chrome\u200b\n\n\nLaunch Google Chrome and navigate to the relevant webpage in Chrome.\n\n\nIn the upper-right corner of your screen, click the three vertical dots.\xa0\n\n\nFrom the Chrome menu, click More Tools and then click Developer Tools.\n\n\nIn the left menu, click on the Network tab and then select Fetch/XHR as your filtering option.', ""From the Chrome menu, click More Tools and then click Developer Tools.\n\n\nIn the left menu, click on the Network tab and then select Fetch/XHR as your filtering option.\n\n\nUnder the Network tab, click Preserve log.\xa0A red circle will appear on the left to show that you have started recording the network log. If you see a black circle, click on it to turn it red and start recording.\n\n\nTo allow Google Chrome to record the interaction between the browser and website, refresh the page. Confirm if you can view new entries in the console.\n\n\nNext to the red circle icon, click the circle slash icon to clear logs.\n\n\nReplicate the issue that you experienced in the browser. For example, if it's a particular click that triggers an error, perform this action so that the error is recorded in the console.\n\n\nOnce the page has loaded, navigate to the Console tab\xa0and right-click in the console box. Select Save as... and enter a name for the file."", ""Hardening configurations through content security policies,\nValidating all inputs,\nRequiring least privileges,\nEmploying defense-in-depth techniques like code obfuscation to frustrate reverse engineering,\nAccessing customer resources over secure HTTPS channels after SSL certificate verification to prevent tampering.\n\nAtlan follows proven CI/CD methodologies used for our SaaS application, enabling rapid and frequent updates to Atlan's Chrome extension. This allows:\n\nPatching identified vulnerabilities faster through new releases while simultaneously upholding stability.\nMandatory code reviews specifically focused on analyzing security to help with identifying issues before these can impact customers."", 'Troubleshooting Atlan browser extension | Atlan Documentation']"
How can you ensure that column data types are correctly displayed for both dbt Cloud and dbt Core models in Atlan?,"For dbt Cloud, you must enable the 'Generate docs on run' option for at least one job run in every environment. For dbt Core, you need to upload the `manifest.json` and `catalog.json` files that are generated by the `dbt docs generate` command.","To ensure that column data types are correctly displayed for both dbt Cloud and dbt Core models in Atlan, you need to follow specific steps for each environment:

1. **For dbt Cloud:** Make sure that there is at least one job run with the ""Generate docs on run"" option enabled for every environment where dbt models are executed. This is necessary for Atlan to correctly crawl and display column data types (Source: https://docs.atlan.com/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-cloud).

2. **For dbt Core:** You must upload the `manifest.json` and `catalog.json` files generated by the `dbt docs generate` command for every dbt Core project. This ensures that Atlan can access the necessary metadata to display the column data types (Source: https://docs.atlan.com/apps/connectors/etl-tools/dbt/references/what-does-atlan-crawl-from-dbt-core).","[""Why are my dbt Cloud models not showing up in Atlan after setup?\u200b\nAtlan only crawls dbt assets that are in the “applied” (built) state in dbt Cloud. Models must be part of a successful run to be picked up during crawling; models that are only defined in your project files but haven’t been executed won’t be included. For more information about project state, see Project states in dbt Cloud.\nDoes Atlan support data types for dbt model columns?\u200b\nYes, Atlan supports column data types for dbt models:\n\ndbt Cloud: Make sure there is at least one job run with the Generate docs on run option enabled for every environment where dbt models are executed.\ndbt Core   -  upload the manifest.json and catalog.json files generated by the dbt docs generate command for every dbt Core project. Refer to dbt Core documentation to learn how to structure the bucket while uploading your files.\n\nWhy are columns for dbt models missing?\u200b\nIf you've crawled your dbt models but columns are missing:"", 'Columns\u200b\nAtlan maps columns from dbt Cloud to its Column asset type.\nSource propertyAtlan propertyWhere in Atlandescriptiondescriptionasset profile and overview sidebar[collected via REST API]assetDbtTestStatusAPI onlyaliasassetDbtAliasasset filter and properties sidebarmetaassetDbtMetaAPI onlyuniqueIdassetDbtUniqueIdasset filter and overview sidebaraccountNameassetDbtAccountNameasset filterprojectNameassetDbtProjectNameasset filter and overview sidebarpackageNameassetDbtPackageNameasset filter and properties sidebartagsassetDbtTagsasset filter and overview sidebar\nModels\u200b\nAtlan maps models from dbt Cloud to its Model asset type.', 'Columns\u200b\nAtlan maps columns from dbt Core to its Column asset type.\nSource propertyAtlan propertyWhere in Atlandescriptiondescriptionasset profile and overview sidebarmetaassetDbtMetaAPI onlytagsassetDbtTagsasset filter and overview sidebarpackageNameassetDbtPackageNameasset filter and properties sidebardescriptiondescriptionasset profile and overview sidebarcreated_atsourceCreatedAtasset profile, overview and properties sidebar\nModels\u200b\nAtlan maps models from dbt Core to its Model asset type.', ""Atlan's dbt connectivity also populates custom metadata to further enrich the assets in Atlan. The Atlan dbt-specific property column in the tables below gives the name of the mapped custom metadata property in Atlan.\nDid you know?Atlan enables you to sync your dbt tags and update your dbt assets with the synced tags. It's also possible to map other metadata on Atlan's assets through your dbt models.\nTables\u200b\nAtlan maps tables from dbt Cloud to its Table asset type.""]"
What are two reasons some dbt tests might be missing or why the test count might differ between Atlan and dbt?,"Some dbt tests might be missing because Atlan does not support tests with an auto-generated `unique_id` longer than 32,000 characters. Additionally, the test count can differ because Atlan fetches assets from the 'applied state' of an environment, which may not match the asset count in the manifest files from individual job runs.","Two reasons some dbt tests might be missing or why the test count might differ between Atlan and dbt are:

1. **Character Limit on Unique IDs**: Atlan doesn't support crawling dbt tests with an auto-generated unique_id that exceeds the character limit of 32,000 characters. To catalog such dbt tests in Atlan, users need to define a custom name for their dbt tests within this character limit (Source: https://docs.atlan.com/apps/connectors/etl-tools/dbt/troubleshooting/troubleshooting-dbt-connectivity).

2. **Processing Limitations**: For dbt Core, Atlan currently only processes the status of dbt tests (passed, failed, and warning) but not the job name and execution time, which is available for dbt Cloud. This can lead to discrepancies in the reported test counts between the two platforms (Source: https://docs.atlan.com/apps/connectors/etl-tools/dbt).","['Why is there a discrepancy in dbt test count between dbt and Atlan?\u200b\nAtlan fetches dbt models, sources, and tests from the applied state of each dbt environment. This is the most recent project state for each environment. The asset count on Atlan may differ from what’s present in the manifest files for individual job runs. Refer to dbt documentation to learn more about dbt project states.\nWhy does Atlan link the dbt model description to my SQL source table?\u200b\nIf a materialized table is linked to multiple dbt assets, Atlan applies the description from the linked dbt model to the materialized asset. In case this is unavailable, Atlan then applies the description from the linked dbt source to the asset.\nWhy does Atlan link the dbt seed description to my SQL source table?\u200b', ""Is dbt source metadata available for materialized columns?\u200b\nIf metadata enrichment is enabled for assets that dbt materializes, only table assets are updated with the materialized by dbt source or model metadata. Materialized columns are only updated with dbt model metadata   -  dbt source metadata is currently not supported for column assets.\nCan I map the Atlan GitHub action to multiple dbt projects?\u200b\nYes, you can configure the Atlan GitHub action for multiple dbt projects.\nDoes Atlan support syncing terms from dbt?\u200b\nYes, you can update terms from dbt to Atlan. For more details, refer to the developer documentation.\nWhy are some dbt tests missing?\u200b\nAtlan doesn't support crawling dbt tests with an auto-generated unique_id that exceeds the character limit of 32,000 characters. If you want to catalog such dbt tests in Atlan, you need to define a custom name for your dbt tests within the character limit.\nWhy is there a discrepancy in dbt test count between dbt and Atlan?\u200b"", ""For dbt Core, Atlan currently only processes the status of dbt tests   -  passed, failed, and warning   -  but not the job name and execution time. However, this information is available for dbt Cloud. You can hover over the test Status in the asset sidebar to view this information.\nColumn-level lineage for dbt models is currently unavailable at source. However, Atlan applies a custom SQL-parsing algorithm to the materialized SQL tables to generate column-level lineage for dbt models. Note that due to the limitations of SQL parsing, Atlan doesn't guarantee generating lineage for all columns.\nEphemeral dbt models don't materialize tables at source. Hence, these doesn't create lineage for the materialized layer or publish dbt model columns in Atlan. This is an expected behavior of ephemeral dbt models."", ""dbt | Atlan Documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSkip to main contentSearchPartner with usWhat's newSupport portalContact supportGet startedConnect dataUse dataBuild governanceConfigure AtlandbtGet StartedCrawl dbt AssetsManage dbt in AtlanImpact AnalysisReferencesTroubleshootingConnect dataETL ToolsdbtOn this pagedbt\nOverview:\nCatalog dbt models, sources, tests, and documentation in Atlan. Gain visibility into lineage, transformations, and governance for your data build tool assets.\n\n\ndbt core\u200b\nGet started\u200b\n\nSet up the connector\nCrawl dbt Core assets\n\nGuides\u200b\n\nManage dbt tags: Manage and sync tags from dbt Core.\nAdd impact analysis in GitHub: Enable impact analysis for dbt Core projects in GitHub.\nAdd impact analysis in GitLab: Enable impact analysis for dbt Core projects in GitLab.\nMigrate from dbt to Atlan Action: Migrate your dbt Core integration to Atlan Action.\nEnrich Atlan through dbt: Update and enrich Atlan metadata using dbt Core.\n\nReferences\u200b""]"
